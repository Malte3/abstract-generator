{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["!pip install -q tika\n","\n","!pip install -q summa"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import urllib.request\n","import lxml.etree as etree\n","import tika\n","from tika import parser\n","import csv\n","import os\n","import re\n","import shutil\n","import torch\n","\n","import torchtext\n","from torchtext.data.utils import get_tokenizer, RandomShuffler\n","from torchtext.data.dataset import check_split_ratio, rationed_split, stratify\n","from summa import summarizer\n","\n","tika.initVM()\n","\n","class PaperAbstractDataset(torchtext.data.Dataset):\n","    \"\"\"Defines a dataset composed of Examples along with its Fields, for paper and abstracts.\n","    \"\"\"\n","    sort_key = None\n","\n","    @classmethod\n","    def splits(cls, search_query = 'all', max_results = 300, start = 0, reduced_words=1000, savepath='data',  split_ratio=0.7, stratified=False, strata_field='abstract',\n","              random_state=None, **kwargs):\n","        \"\"\"Create Dataset objects for multiple splits of a dataset.\n","\n","        Arguments:\n","            search_query (str): specify searchh query for arxiv, default is 'all' results, more information on ttps://arxiv.org/help/api.\n","            max_results (int): maxium search results from arxiv from search query\n","            savepath (str): save path for the txt files\n","            split_ratio (float or List of floats): a number [0, 1] denoting the amount\n","                of data to be used for the training split (rest is used for validation),\n","                or a list of numbers denoting the relative sizes of train, test and valid\n","                splits respectively. If the relative size for valid is missing, only the\n","                train-test split is returned. Default is 0.7 (for the train set).\n","            stratified (bool): whether the sampling should be stratified.\n","                Default is False.\n","            strata_field (str): name of the examples Field stratified over.\n","                Default is 'label' for the conventional label field.\n","            random_state (tuple): the random seed used for shuffling.\n","                A return value of `random.getstate()`.\n","\n","        Returns:\n","            Tuple[Dataset]: Datasets for train, validation, and\n","            test splits in that order, if provided.\n","        \"\"\"\n","\n","         # initialize text field\n","        text_field = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"), init_token='<sos>', eos_token='<eos>', lower=True)\n","        fields = [('abstract', text_field), ('paper', text_field)]\n","        examples = []\n","\n","        # Create new dataset by downloading from arxiv or open dataset from folder\n","        if not os.path.exists(savepath):\n","            os.mkdir(savepath)\n","            # create directories for saving the data set\n","            if not os.path.exists(os.path.join(savepath, 'temp')):\n","                os.mkdir(os.path.join(savepath, 'temp'))\n","            if not os.path.exists(os.path.join(savepath, 'abstracts')):\n","                os.mkdir(os.path.join(savepath, 'abstracts'))\n","            if not os.path.exists(os.path.join(savepath, 'paper')):\n","                os.mkdir(os.path.join(savepath, 'paper'))\n","            data = cls.download(search_query=search_query, max_results=max_results)\n","            abstracts, papers = cls.extract_paper_and_abstract(data, savepath=savepath)\n","\n","            # generate all examples\n","            for i, (abstract, paper) in enumerate(zip(abstracts, papers)):\n","                paper_tokenized = []\n","                abstract_tokenized = []\n","                # rdeduce the number of words with the textrank approach\n","                textranked_paper = summarizer.summarize(paper, words=reduced_words)\n","                # add start and end token\n","                paper_tokenized += [u'<sos>'] + text_field.preprocess(textranked_paper) + [u'<eos>']\n","                abstract_tokenized += [u'<sos>'] + text_field.preprocess(abstract) + [u'<eos>']\n","                # initialize examples\n","                examples.append(torchtext.data.Example.fromlist([abstract_tokenized, paper_tokenized], fields))\n","\n","                # save data samples in txt files\n","                with open(os.path.join(savepath,'abstracts','abstract_' + str(i) + '.txt'), 'w+', encoding='utf-8') as abstr:\n","                    csvwriter = csv.writer(abstr, delimiter=' ')\n","                    csvwriter.writerow(abstract_tokenized)\n","                with open(os.path.join(savepath,'paper','paper_' + str(i) + '.txt'), 'w+', encoding='utf-8') as pap:\n","                    csvwriter = csv.writer(pap, delimiter=' ')\n","                    csvwriter.writerow(paper_tokenized)\n","\n","        else:\n","            # read all files in saved data path\n","            paper_files = os.listdir(os.path.join(savepath,'paper'))\n","            abstract_files = os.listdir(os.path.join(savepath,'abstracts'))\n","            data = [[paper, abstract] for paper, abstract in zip(paper_files, abstract_files)]\n","            papers_tokenized = []\n","            abstracts_tokenized = []\n","            # read paper and abstracts from files\n","            for paper, abstract in data:\n","                with open(os.path.join(savepath, 'paper', paper), encoding='utf-8') as csvfile:\n","                    paper = csv.reader(csvfile, delimiter=' ')\n","                    for pap in paper:\n","                        if pap:\n","                            papers_tokenized.append(pap)\n","                with open(os.path.join(savepath, 'abstracts', abstract), encoding='utf-8') as csvfile:\n","                    abstract = csv.reader(csvfile, delimiter=' ')\n","                    for abstr in abstract:\n","                        if abstr:\n","                            abstracts_tokenized.append(abstr)\n","       \n","            # generate all examples\n","            for abstract_tokenized, paper_tokenized in zip(abstracts_tokenized, papers_tokenized):\n","                examples.append(torchtext.data.Example.fromlist([abstract_tokenized, paper_tokenized], fields))\n","\n","        # create initial dataset\n","        dataset = PaperAbstractDataset(examples, fields)\n","        # split dataset\n","        splits = dataset.split(split_ratio=split_ratio, stratified=stratified, strata_field=strata_field,\n","              random_state=random_state)\n","        # initialize vocabulary\n","        pre_trained_vector_type = 'glove.6B.300d'\n","        for d in splits:\n","            for name, field in d.fields.items():\n","                field.build_vocab(splits[0], vectors=pre_trained_vector_type)\n","            d.filter_examples(['abstract', 'paper'])\n","        return splits\n","\n","    @classmethod\n","    def download(cls, search_query = 'all', max_results = 300, start = 0):\n","        '''\n","            Download e-prints from https://arxiv.org with arXiv API\n","            search_query (str): specify searchh query for arxiv, default is 'all' results, more information on ttps://arxiv.org/help/api.\n","            max_results (int): maxium search results from arxiv from search query\n","        '''\n","        url = 'http://export.arxiv.org/api/query?search_query=' + search_query + '&start=' + str(start) + '&max_results=' + str(max_results)\n","        data = urllib.request.urlopen(url).read()\n","        return data\n","\n","    @classmethod\n","    def extract_paper_and_abstract(cls, data, savepath='.'):\n","        '''\n","            Extract the abstracts from the xml search query response and download the pdf paper and extract the plain text from it and remove possible abstract in there\n","\n","            data (str): xml data with all paper urls and abstracts\n","            savepath (str): save path for the txt files\n","        '''\n","        # build xml tree\n","        root = etree.fromstring(data)\n","        # reserve lists\n","        abstracts = []\n","        papers = []\n","        # extract abstract directly from summary tag and extract pdf url\n","        for child in root:\n","            if len(child) > 0 and child.tag == '{http://www.w3.org/2005/Atom}entry':\n","                for grandchild in child:\n","                    if grandchild.tag == '{http://www.w3.org/2005/Atom}summary':\n","                            abstracts.append(grandchild.text)\n","                    if grandchild.tag == '{http://www.w3.org/2005/Atom}link' and 'title' in grandchild.attrib and grandchild.attrib['title'] == 'pdf':\n","                            papers.append(grandchild.attrib['href'])\n","        \n","        ## download pdfs, extract plain text, remove possible abstracts in there\n","        for i, paper in enumerate(papers):\n","            # download pdf\n","            pdf = urllib.request.urlopen(paper).read()\n","            # save pdf temporarily as file\n","            filename = paper.split('/')[-1]\n","            with open(os.path.join(savepath,'temp', filename + '.pdf'), 'wb+') as f:\n","                    f.write(pdf)\n","            # parse pdf file to get psdf text content and replace url with text content\n","            parsed = parser.from_file(os.path.join(savepath,'temp', filename + '.pdf'))['content']\n","            # remove line breaks with hyphenation in paper and abstract\n","            hyph_norm_parsed_paper = re.sub('-\\n', '', parsed)\n","            abstract_rgex_pattern = re.sub('-\\n', '', abstracts[i])\n","            # make list of words from abstract\n","            abstract_words = abstract_rgex_pattern.split()\n","            # remove abstract by searching for paragraph starting with abstract and ending with double whitespace\n","            parsed = re.sub('(?i)(\\s*)(abstract)([\\s\\S]' + '{0,' + str(len(abstract_rgex_pattern)*4//3) + '})' + re.escape(abstract_rgex_pattern[-3:-1]) + '\\s\\s', '\\n', hyph_norm_parsed_paper)\n","            # remove abstract by searching for paragraph start and end of abstract\n","            parsed = re.sub('(?i)(\\s*)' + re.escape(abstract_rgex_pattern[0:3]) + '([\\s\\S]' + '{0,' + str(len(abstract_rgex_pattern)*4//3) + '})' + re.escape(abstract_rgex_pattern[-3:-1]) , '\\n', parsed)\n","            # remove abstract by searching for paragraph start word and end word of abstract\n","            parsed = re.sub('(?i)(\\s*)' + re.escape(abstract_words[0]) + '([\\s\\S]' + '{0,' + str(len(abstract_rgex_pattern)*4//3) + '})' + re.escape(abstract_words[-1]), '\\n', parsed)\n","            # remove abstract heading in case it was not found before\n","            parsed = re.sub('(?i)(\\s*)(abstract)', '\\n', parsed)\n","            # remove unnecessary whitespace\n","            abstracts[i] = ' '.join(abstract_words)\n","            parsed = ' '.join(parsed.split())\n","            papers[i] = parsed\n","\n","        # remove temporary pdfs\n","        shutil.rmtree(os.path.join(savepath,'temp'))\n","        return abstracts, papers"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"5%|▌         | 21368/400000 [00:04<01:25, 4448.83it/s]\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-11-19e05f117b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Dataset inherited from torchtext.data.Dataset either creates data by downloading from arxiv, 500 is maximum ofd samples and the number of words needs to be reduced to fit inside the memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPaperAbstractDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Initialize Interator for the data with batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-10-22bc28c006aa>\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, search_query, max_results, start, reduced_words, savepath, split_ratio, stratified, strata_field, random_state, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_trained_vector_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'paper'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m                             self.eos_token] + kwargs.pop('specials', [])\n\u001b[0;32m    308\u001b[0m             if tok is not None))\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspecials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munk_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectors_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0munk_init\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvectors_cache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36mload_vectors\u001b[1;34m(self, vectors, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m                         \"vectors are {}\".format(\n\u001b[0;32m    181\u001b[0m                             vector, list(pretrained_aliases.keys())))\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_aliases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 raise ValueError(\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'glove.{}.{}d.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mL:\\Miniconda3\\envs\\AbstractGenerator\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36mcache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    413\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                     \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectors_loaded\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m                     \u001b[0mvectors_loaded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                     \u001b[0mitos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","import math\n","import torch\n","import torchtext\n","import random\n","import os\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","# initialize random seed so the train, evaluation and test split stay the same\n","random.seed(42)\n","\n","# Do computations on gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Dataset inherited from torchtext.data.Dataset either creates data by downloading from arxiv, 500 is maximum ofd samples and the number of words needs to be reduced to fit inside the memory\n","train_data, val_data = PaperAbstractDataset.splits(max_results=5, reduced_words=500, random_state=random.getstate())\n","\n","# Initialize Interator for the data with batch size\n","batch_size = 1\n","train_iter, val_iter = torchtext.data.BucketIterator.splits(\n","                        (train_data, val_data), batch_sizes=(batch_size, batch_size),\n","                        device=device, \n","                        sort_key=lambda x: len(x.paper),\n","                        shuffle=True, sort_within_batch=False, repeat=False)\n","# Put paper and abstract (source, target) in one batch tuple\n","class BatchTuple():\n","    def __init__(self, dataset, x_var, y_var):\n","        self.dataset, self.x_var, self.y_var = dataset, x_var, y_var\n","        \n","    def __iter__(self):\n","        for batch in self.dataset:\n","            x = getattr(batch, self.x_var) \n","            y = getattr(batch, self.y_var)                 \n","            yield (x, y)\n","            \n","    def __len__(self):\n","        return len(self.dataset)\n","\n","train_iter_tuple = BatchTuple(train_iter, \"abstract\", \"paper\")\n","val_iter_tuple = BatchTuple(val_iter, \"abstract\", \"paper\")\n","\n","# put out one example batch tuple\n","next(iter(train_iter_tuple))\n","\n","# compute maximal paper length\n","MAX_LENGTH = 0\n","for abstract, paper in train_iter_tuple:\n","    if len(paper) > MAX_LENGTH:\n","        MAX_LENGTH = len(paper)\n","for abstract, paper in val_iter_tuple:\n","    if len(paper) > MAX_LENGTH:\n","        MAX_LENGTH = len(paper)\n","\n","# The encoder and decoder model from NLP From Scratch: Translation with a Sequence to Sequence Network and Attention (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), chosen because of its simplicity\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","# Normal Decoder\n","class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","# Attention Decoder\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","teacher_forcing_ratio = 0.5\n","\n","# The training process is also from NLP From Scratch: Translation with a Sequence to Sequence Network and Attention (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        decoder_input = target_tensor[0]\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def trainIters(encoder, decoder, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","    for epoch in range(epochs):\n","        for target, source in train_iter_tuple:\n","            input_tensor = source\n","            target_tensor = target\n","\n","            loss = train(input_tensor, target_tensor, encoder,\n","                        decoder, encoder_optimizer, decoder_optimizer, criterion)\n","            print_loss_total += loss\n","            plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, (epoch+1) / epochs),\n","                                        epoch, epoch / epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = sentence\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                     encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = sentence[0]  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_attentions[di] = decoder_attention.data\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == train_data.fields['paper'].eos_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                if topi.item() != train_data.fields['paper'].pad_token:\n","                    decoded_words.append(topi.item())\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return torch.tensor(decoded_words, dtype=torch.int32).unsqueeze(-1), decoder_attentions[:di + 1]\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","\n","# function to make words from numericalized text adapted from torchtext ReverseField\n","def reverse(field, batch):\n","        if not field.batch_first:\n","            batch = batch.t()\n","        with torch.cuda.device_of(batch):\n","            batch = batch.tolist()\n","        batch = [[field.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n","\n","        def trim(s, t):\n","            sentence = []\n","            for w in s:\n","                if w == t:\n","                    break\n","                sentence.append(w)\n","            return sentence\n","\n","        batch = [trim(ex, field.eos_token) for ex in batch]  # trim past frst eos\n","\n","        def filter_special(tok):\n","            return tok not in (field.init_token, field.pad_token)\n","\n","        batch = [filter(filter_special, ex) for ex in batch]\n","        return [' '.join(ex) for ex in batch]\n","\n","def evaluateRandomly(encoder, decoder, n=2):\n","    for i in range(n):\n","        pair = next(iter(val_iter_tuple))\n","        print('>', reverse(train_data.fields['paper'], pair[0]))\n","        print('=', reverse(train_data.fields['paper'], pair[1]))\n","        output_words, attentions = evaluate(encoder, decoder, pair[1])\n","        output_sentence = reverse(train_data.fields['paper'], output_words)\n","        print('<', output_sentence)\n","        print('')\n","\n","# Initialize Network and Parameters\n","hidden_size = 256\n","ntokens = len(next(iter(train_data.fields.values())).vocab.stoi) \n","encoder1 = EncoderRNN(ntokens, hidden_size).to(device)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, ntokens, dropout_p=0.1).to(device)\n","\n","# Train new parameters or load them from previous trainings, if you want to restart training delte or rename old .pth files \n","if os.path.exists('encoder.pth') and os.path.exists('decoder.pth'):\n","    encoder1.load_state_dict(torch.load('encoder.pth'))\n","    attn_decoder1.load_state_dict(torch.load('decoder.pth'))\n","else:\n","    trainIters(encoder1, attn_decoder1, 250, print_every=1)\n","    torch.save(encoder1.state_dict(), 'encoder.pth')\n","    torch.save(attn_decoder1.state_dict(), 'decoder.pth')\n","\n","# Show some randomly chosen evaluations\n","evaluateRandomly(encoder1, attn_decoder1)\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["#!rm -rf /content/data"]}]}